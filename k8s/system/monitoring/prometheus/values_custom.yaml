#
# Custom overrides for the helm chart
# k3s
#

alertmanager:
  persistentVolume:
    ## alertmanager data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteMany

    ## alertmanager data Persistent Volume existing claim name
    ## Requires alertmanager.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: "pvc-alertmanager"

    ## alertmanager data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: "-"

  ## alertmanager resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 10m
      memory: 50Mi
    requests:
      cpu: 10m
      memory: 50Mi

  # service:
  #   annotations: {}
  #   labels: {}
  #   clusterIP: ""

  #   ## Enabling peer mesh service end points for enabling the HA alert manager
  #   ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md
  #   # enableMeshPeer : true

  #   ## List of IP addresses at which the alertmanager service is available
  #   ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  #   ##
  #   externalIPs: []

  #   loadBalancerIP: ""
  #   loadBalancerSourceRanges: []
  #   servicePort: 80
  #   # nodePort: 30000
  #   sessionAffinity: None
  #   type: ClusterIP

kubeStateMetrics:
  ## kube-state-metrics container image
  # use my own since there is no official ARM image
  ##
  image:
    repository: rhuss/kube-state-metrics
    tag: v1.9.3
    pullPolicy: IfNotPresent

  ## kube-state-metrics resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 10m
      memory: 16Mi
    requests:
      cpu: 10m
      memory: 16Mi

  # service:
  #   annotations:
  #     prometheus.io/scrape: "true"
  #   labels: {}

  #   # Exposed as a headless service:
  #   # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
  #   clusterIP: None

  #   ## List of IP addresses at which the kube-state-metrics service is available
  #   ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  #   ##
  #   externalIPs: []

  #   loadBalancerIP: ""
  #   loadBalancerSourceRanges: []
  #   servicePort: 80
  #   # Port for Kubestatemetric self telemetry
  #   serviceTelemetryPort: 81
  #   type: ClusterIP

nodeExporter:
  ## node-exporter resource limits & requests
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 200m
      memory: 50Mi
    requests:
      cpu: 100m
      memory: 30Mi

  # service:
  #   annotations:
  #     prometheus.io/scrape: "true"
  #   labels: {}

  #   # Exposed as a headless service:
  #   # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
  #   clusterIP: None

  #   ## List of IP addresses at which the node-exporter service is available
  #   ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  #   ##
  #   externalIPs: []

  #   hostPort: 9100
  #   loadBalancerIP: ""
  #   loadBalancerSourceRanges: []
  #   servicePort: 9100
  #   type: ClusterIP

server:
  persistentVolume:

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteMany

    ## Prometheus server data Persistent Volume annotations
    ##
    # annotations: {}

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: "pvc-prometheus"

    ## Prometheus server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: "-"

  ## Prometheus server readiness and liveness probe initial delay and timeout
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  ##
  # readinessProbeInitialDelay: 30
  # readinessProbeTimeout: 30
  # readinessProbeFailureThreshold: 3
  # readinessProbeSuccessThreshold: 1
  # livenessProbeInitialDelay: 30
  # livenessProbeTimeout: 30
  # livenessProbeFailureThreshold: 3
  # livenessProbeSuccessThreshold: 1

  ## Prometheus server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: 
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 500m
      memory: 512Mi

  # service:
  #   annotations: {}
  #   labels: {}
  #   clusterIP: ""

  #   ## List of IP addresses at which the Prometheus server service is available
  #   ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  #   ##
  #   externalIPs: []

  #   loadBalancerIP: ""
  #   loadBalancerSourceRanges: []
  #   servicePort: 80
  #   sessionAffinity: None
  #   type: ClusterIP


  ## Prometheus server pod termination grace period
  ##
  terminationGracePeriodSeconds: 300

  ## Prometheus data retention period (default if not specified is 15 days)
  ##
  retention: "60d"

pushgateway:
  ## pushgateway resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 10m
      memory: 32Mi
    requests:
      cpu: 10m
      memory: 32Mi

  # service:
  #   annotations:
  #     prometheus.io/probe: pushgateway
  #   labels: {}
  #   clusterIP: ""

  #   ## List of IP addresses at which the pushgateway service is available
  #   ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
  #   ##
  #   externalIPs: []

  #   loadBalancerIP: ""
  #   loadBalancerSourceRanges: []
  #   servicePort: 9091
  #   type: ClusterIP

  # ## pushgateway Deployment Strategy type
  # # strategy:
  # #   type: Recreate

  persistentVolume:
    ## If true, pushgateway will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## pushgateway data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteMany

    ## pushgateway data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## pushgateway data Persistent Volume existing claim name
    ## Requires pushgateway.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: "pvc-pushgateway"


    ## pushgateway data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: "-"


#TODO - maybe revisit

## Prometheus server ConfigMap entries
##
# serverFiles:

#   ## Alerts configuration
#   ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
#   alerting_rules.yml: {}
#   # groups:
#   #   - name: Instances
#   #     rules:
#   #       - alert: InstanceDown
#   #         expr: up == 0
#   #         for: 5m
#   #         labels:
#   #           severity: page
#   #         annotations:
#   #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
#   #           summary: 'Instance {{ $labels.instance }} down'
#   ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml
#   alerts: {}

#   ## Records configuration
#   ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
#   recording_rules.yml: {}
#   ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml
#   rules: {}

#   prometheus.yml:
#     rule_files:
#       - /etc/config/recording_rules.yml
#       - /etc/config/alerting_rules.yml
#     ## Below two files are DEPRECATED will be removed from this default values file
#       - /etc/config/rules
#       - /etc/config/alerts

#     scrape_configs:
#       - job_name: prometheus
#         static_configs:
#           - targets:
#             - localhost:9090

#       # A scrape configuration for running Prometheus on a Kubernetes cluster.
#       # This uses separate scrape configs for cluster components (i.e. API server, node)
#       # and services to allow each to use different authentication configs.
#       #
#       # Kubernetes labels will be added as Prometheus labels on metrics via the
#       # `labelmap` relabeling action.

#       # Scrape config for API servers.
#       #
#       # Kubernetes exposes API servers as endpoints to the default/kubernetes
#       # service so this uses `endpoints` role and uses relabelling to only keep
#       # the endpoints associated with the default/kubernetes service using the
#       # default named port `https`. This works for single API server deployments as
#       # well as HA API server deployments.
#       - job_name: 'kubernetes-apiservers'

#         kubernetes_sd_configs:
#           - role: endpoints

#         # Default to scraping over https. If required, just disable this or change to
#         # `http`.
#         scheme: https

#         # This TLS & bearer token file config is used to connect to the actual scrape
#         # endpoints for cluster components. This is separate to discovery auth
#         # configuration because discovery & scraping are two separate concerns in
#         # Prometheus. The discovery auth config is automatic if Prometheus runs inside
#         # the cluster. Otherwise, more config options have to be provided within the
#         # <kubernetes_sd_config>.
#         tls_config:
#           ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
#           # If your node certificates are self-signed or use a different CA to the
#           # master CA, then disable certificate verification below. Note that
#           # certificate verification is an integral part of a secure infrastructure
#           # so this should only be disabled in a controlled environment. You can
#           # disable certificate verification by uncommenting the line below.
#           #
#           insecure_skip_verify: true
#         bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

#         # Keep only the default/kubernetes service endpoints for the https port. This
#         # will add targets for each API server which Kubernetes adds an endpoint to
#         # the default/kubernetes service.
#         relabel_configs:
#           - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
#             action: keep
#             regex: default;kubernetes;https

#       - job_name: 'kubernetes-nodes'

#         # Default to scraping over https. If required, just disable this or change to
#         # `http`.
#         scheme: https

#         # This TLS & bearer token file config is used to connect to the actual scrape
#         # endpoints for cluster components. This is separate to discovery auth
#         # configuration because discovery & scraping are two separate concerns in
#         # Prometheus. The discovery auth config is automatic if Prometheus runs inside
#         # the cluster. Otherwise, more config options have to be provided within the
#         # <kubernetes_sd_config>.
#         tls_config:
#           ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
#           # If your node certificates are self-signed or use a different CA to the
#           # master CA, then disable certificate verification below. Note that
#           # certificate verification is an integral part of a secure infrastructure
#           # so this should only be disabled in a controlled environment. You can
#           # disable certificate verification by uncommenting the line below.
#           #
#           insecure_skip_verify: true
#         bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

#         kubernetes_sd_configs:
#           - role: node

#         relabel_configs:
#           - action: labelmap
#             regex: __meta_kubernetes_node_label_(.+)
#           - target_label: __address__
#             replacement: kubernetes.default.svc:443
#           - source_labels: [__meta_kubernetes_node_name]
#             regex: (.+)
#             target_label: __metrics_path__
#             replacement: /api/v1/nodes/$1/proxy/metrics


#       - job_name: 'kubernetes-nodes-cadvisor'

#         # Default to scraping over https. If required, just disable this or change to
#         # `http`.
#         scheme: https

#         # This TLS & bearer token file config is used to connect to the actual scrape
#         # endpoints for cluster components. This is separate to discovery auth
#         # configuration because discovery & scraping are two separate concerns in
#         # Prometheus. The discovery auth config is automatic if Prometheus runs inside
#         # the cluster. Otherwise, more config options have to be provided within the
#         # <kubernetes_sd_config>.
#         tls_config:
#           ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
#           # If your node certificates are self-signed or use a different CA to the
#           # master CA, then disable certificate verification below. Note that
#           # certificate verification is an integral part of a secure infrastructure
#           # so this should only be disabled in a controlled environment. You can
#           # disable certificate verification by uncommenting the line below.
#           #
#           insecure_skip_verify: true
#         bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

#         kubernetes_sd_configs:
#           - role: node

#         # This configuration will work only on kubelet 1.7.3+
#         # As the scrape endpoints for cAdvisor have changed
#         # if you are using older version you need to change the replacement to
#         # replacement: /api/v1/nodes/$1:4194/proxy/metrics
#         # more info here https://github.com/coreos/prometheus-operator/issues/633
#         relabel_configs:
#           - action: labelmap
#             regex: __meta_kubernetes_node_label_(.+)
#           - target_label: __address__
#             replacement: kubernetes.default.svc:443
#           - source_labels: [__meta_kubernetes_node_name]
#             regex: (.+)
#             target_label: __metrics_path__
#             replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

#       # Scrape config for service endpoints.
#       #
#       # The relabeling allows the actual service scrape endpoint to be configured
#       # via the following annotations:
#       #
#       # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
#       # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
#       # to set this to `https` & most likely set the `tls_config` of the scrape config.
#       # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
#       # * `prometheus.io/port`: If the metrics are exposed on a different port to the
#       # service then set this appropriately.
#       - job_name: 'kubernetes-service-endpoints'

#         kubernetes_sd_configs:
#           - role: endpoints

#         relabel_configs:
#           - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
#             action: keep
#             regex: true
#           - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
#             action: replace
#             target_label: __scheme__
#             regex: (https?)
#           - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
#             action: replace
#             target_label: __metrics_path__
#             regex: (.+)
#           - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
#             action: replace
#             target_label: __address__
#             regex: ([^:]+)(?::\d+)?;(\d+)
#             replacement: $1:$2
#           - action: labelmap
#             regex: __meta_kubernetes_service_label_(.+)
#           - source_labels: [__meta_kubernetes_namespace]
#             action: replace
#             target_label: kubernetes_namespace
#           - source_labels: [__meta_kubernetes_service_name]
#             action: replace
#             target_label: kubernetes_name
#           - source_labels: [__meta_kubernetes_pod_node_name]
#             action: replace
#             target_label: kubernetes_node

#       # Scrape config for slow service endpoints; same as above, but with a larger
#       # timeout and a larger interval
#       #
#       # The relabeling allows the actual service scrape endpoint to be configured
#       # via the following annotations:
#       #
#       # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
#       # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
#       # to set this to `https` & most likely set the `tls_config` of the scrape config.
#       # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
#       # * `prometheus.io/port`: If the metrics are exposed on a different port to the
#       # service then set this appropriately.
#       - job_name: 'kubernetes-service-endpoints-slow'

#         scrape_interval: 5m
#         scrape_timeout: 30s

#         kubernetes_sd_configs:
#           - role: endpoints

#         relabel_configs:
#           - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
#             action: keep
#             regex: true
#           - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
#             action: replace
#             target_label: __scheme__
#             regex: (https?)
#           - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
#             action: replace
#             target_label: __metrics_path__
#             regex: (.+)
#           - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
#             action: replace
#             target_label: __address__
#             regex: ([^:]+)(?::\d+)?;(\d+)
#             replacement: $1:$2
#           - action: labelmap
#             regex: __meta_kubernetes_service_label_(.+)
#           - source_labels: [__meta_kubernetes_namespace]
#             action: replace
#             target_label: kubernetes_namespace
#           - source_labels: [__meta_kubernetes_service_name]
#             action: replace
#             target_label: kubernetes_name
#           - source_labels: [__meta_kubernetes_pod_node_name]
#             action: replace
#             target_label: kubernetes_node

#       - job_name: 'prometheus-pushgateway'
#         honor_labels: true

#         kubernetes_sd_configs:
#           - role: service

#         relabel_configs:
#           - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
#             action: keep
#             regex: pushgateway

#       # Example scrape config for probing services via the Blackbox Exporter.
#       #
#       # The relabeling allows the actual service scrape endpoint to be configured
#       # via the following annotations:
#       #
#       # * `prometheus.io/probe`: Only probe services that have a value of `true`
#       - job_name: 'kubernetes-services'

#         metrics_path: /probe
#         params:
#           module: [http_2xx]

#         kubernetes_sd_configs:
#           - role: service

#         relabel_configs:
#           - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
#             action: keep
#             regex: true
#           - source_labels: [__address__]
#             target_label: __param_target
#           - target_label: __address__
#             replacement: blackbox
#           - source_labels: [__param_target]
#             target_label: instance
#           - action: labelmap
#             regex: __meta_kubernetes_service_label_(.+)
#           - source_labels: [__meta_kubernetes_namespace]
#             target_label: kubernetes_namespace
#           - source_labels: [__meta_kubernetes_service_name]
#             target_label: kubernetes_name

#       # Example scrape config for pods
#       #
#       # The relabeling allows the actual pod scrape endpoint to be configured via the
#       # following annotations:
#       #
#       # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
#       # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
#       # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
#       - job_name: 'kubernetes-pods'

#         kubernetes_sd_configs:
#           - role: pod

#         relabel_configs:
#           - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
#             action: keep
#             regex: true
#           - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
#             action: replace
#             target_label: __metrics_path__
#             regex: (.+)
#           - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
#             action: replace
#             regex: ([^:]+)(?::\d+)?;(\d+)
#             replacement: $1:$2
#             target_label: __address__
#           - action: labelmap
#             regex: __meta_kubernetes_pod_label_(.+)
#           - source_labels: [__meta_kubernetes_namespace]
#             action: replace
#             target_label: kubernetes_namespace
#           - source_labels: [__meta_kubernetes_pod_name]
#             action: replace
#             target_label: kubernetes_pod_name

#       # Example Scrape config for pods which should be scraped slower. An useful example
#       # would be stackriver-exporter which querys an API on every scrape of the pod
#       #
#       # The relabeling allows the actual pod scrape endpoint to be configured via the
#       # following annotations:
#       #
#       # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`
#       # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
#       # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
#       - job_name: 'kubernetes-pods-slow'

#         scrape_interval: 5m
#         scrape_timeout: 30s

#         kubernetes_sd_configs:
#           - role: pod

#         relabel_configs:
#           - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
#             action: keep
#             regex: true
#           - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
#             action: replace
#             target_label: __metrics_path__
#             regex: (.+)
#           - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
#             action: replace
#             regex: ([^:]+)(?::\d+)?;(\d+)
#             replacement: $1:$2
#             target_label: __address__
#           - action: labelmap
#             regex: __meta_kubernetes_pod_label_(.+)
#           - source_labels: [__meta_kubernetes_namespace]
#             action: replace
#             target_label: kubernetes_namespace
#           - source_labels: [__meta_kubernetes_pod_name]
#             action: replace
#             target_label: kubernetes_pod_name
